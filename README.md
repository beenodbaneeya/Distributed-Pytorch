# Project Description:
This project demonstrates how to implement Distributed Data Parallel (DDP) in PyTorch to efficiently scale deep learning training across multiple GPUs and nodes 
in an HPC environment. It provides a ready-to-run template for leveraging NCCL-backed multi-GPU communication and distributed data loading.
The code supports both single-node (multi-GPU) and multi-node SLURM clusters, helping users transition from local development to large-scale distributed 
training with minimal modifications.
